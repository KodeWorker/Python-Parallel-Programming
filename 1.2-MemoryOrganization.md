## Memory Organization

### Memory Organization - An Introduction
- Memory organization - The way in which the data is access
- The main problem - Making the response time of the memory compatible with the speed of the processor
- When the processor starts transferring data, the memory will remain occupied for the entire time of the **memory cycle**
- Memory cycle time is defined as the time elapsed bewteen two successive operations
- During data transferring, no device, I/O driver, processor or even processor itself may request the use of memory.
Because the memory is commited to response to the original request.

### The Memory Organization in MIMD Architecture

    [ MIMD ]
        ├───────────────────────────────────────────────┐
    [ Distributed Memory ]                      [ Shared Memory ]
        ├──────────────────┐                            ├──────────┬────────────┬──────────┐
    [ MPP ]     [ Cluster of Workstation]           [ UMA ]     [ NUMA ]    [ NORMA ]   [ COMA ]

### Shared Versus Distributed Memory
- If a processor were to execute the instruction [load R0, i], which means load in the R0 register the contents of the memory location i, the question now is what should happen

|Shared Memory|Distributed Memory|
|:-|:-|
|The **'i'** index is a global address and the memory location **'i'** is the same for each processor| i is a local address|

If two processors were to load the statement R0 at the same time

|Shared Memory|Distributed Memory|
|:-|:-|
|Thet would load the same information in their register R0|Different values may end up in the respective register's R0|

- Shared Memory is sufficient to build a data structure and memory to go to a parallel sub routine; Distributed memory must copy the shared data into each local memory (sometimes the data could be relatively large and take a lot of transfer time).

### Shared Memory Architecture Schema

    [ Processor ]   [ Processor ]   [ Processor ]   [ Processor ]
        │               │               │               │
    [ cache ]       [ cache ]       [ cache ]       [ cache ]
        │               │               │               │
        └=======┬=======┴===============┴=======┬=======┘
                │                               │
            [ Main Memory]                  [ I/O System]

- Problem occurs when the processor modifying the value stored in the memory system is simultaneously used by another processors (cache coherency/coherence).

### Features of Shared Memory Systems
- The memory is the same for all processors
- The synchronization is made possible by controlling the access of processors to the shared memory.
- A shared memory location must not be changed
- Sharing data is fast

### The Memory Access in Shared Memory Systems
- Uniform Memory Access (UMA):
uniform access time for each processor (aka symmetric multiprocessor).
It's easy to implement, but not scalable (Programs manage the resources).
- Non-Uniform Memory Access (NUMA):
Divide memory into two areas for high/low speed memory access (aka distribusted shared memory).
- No Remote Memory Access (NORMA):
Physically distributed among processors, all local memory are private and uses message passing protocol.
- Cache Only Memory Access (COMA):
COMA is similar to NORMA, but it removes duplicates and keeps only the cache memory.

## Reference:

1. **Python Parallel Programming Solutions**
by Giancarlo Zaccone, Packt Publishing, 2016

2. **Python High Performance - Second Edition**
by Gabriele Lanaro, Packt Publishing, May 2017
