## Evaluating the Performance of a Parallel Program
- Analyzing the performance with the help of performance indexes (speedup, efficiency and scaling)
- Amdahl's law Gustafson's law

## Performance of a Parallel Program
- The development of parallel programming created the need of performance metrics and a software tool
- The focus of parallel computing is to solve large problems in a relatively short time
- The performance is achieved by analyzing and quantifying the number of threads and/or the number of processes used
- To analyze this, a few performance indexes are introduced: Speedup, efficiency, and scaling

## Speedup
- It is the messure that displays the benefit of solving a problem in parallel
- It is defined as the ratio of the time taken to solve a problem on a single processing element, T<sub>s</sub>, to the time required to solve the same problem on p identical processing elements, T<sub>p</sub>.
- We denote speedup by **S = T<sub>s</sub> / T<sub>p</sub>**
- The speedup is absolute when T<sub>s</sub> is the execution time of the best sequential algorithm
- The speedup is relative when T<sub>s</sub> is the execution time of the parallel algorithm for single processor

## Recap
- S = p is linear or ideal speedup
- S < p is real speedup
- S > p is super-linear speedup

## Efficiency
- A parallel system with 'p' processing elements can give us a speedup equal to p (hardly achieved)
- Efficiency is a performance metric estimating how well-ultilized the processors are in solving a task
- We denote it by E and can define it as **E = S / p = T<sub>s</sub> / (p * T<sub>p</sub>)**
- The algorithms with linear speedup have the value of E = 1; in other cases, the value of E is less than 1

## Recap
- When E = 1, it is a linear case
- When E < 1, it is a real case
- When E << 1, it is a problem that is parallelizable with low efficiency

## Scaling
- Scaling is defined as the ability to be efficient on a prarllel machine
- It identifies the computing power in proportion with the number of processors
- The scalable system may maintain the same efficiency or improve it, as the problems and processors increase.

## Amdahl's Law
- It is used to design processors and parallel algorithms
- It states that the maximum speedup that can be achieved is limited by the serial component **S= 1 / (1 - P)** whane 1 - P denotes the serial component (cannot be parallelized) of a program

## Gustafson's Law
- While increasing the dimension of a probelm, its sequential parts remain constant
- While increasing the number of processors, the work required on each of them still remains the same
- **S(P) = P - α (P - 1)**, where P is the number of processors, S is the speedup, and α is the non-parallelizable fraction of any parallel process
- Gustafson's law addresses the deficiency of Amdahl's law

## Reference:

1. **Python Parallel Programming Solutions**
by Giancarlo Zaccone, Packt Publishing, 2016

2. **Python High Performance - Second Edition**
by Gabriele Lanaro, Packt Publishing, May 2017
