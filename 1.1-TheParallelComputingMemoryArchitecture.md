## The Parallel Computing Memory Architecture

### Classifications
- SISD, MISD, SIMD and MIMD

|||
|---|---|
|Single Instruction Single Data (SISD)|Multiple Instructions Single Data (MISD)|
| Single Instruction Multiple Data (SIMD)| Multiple Instructions Multiple Data (MIMD)|
||||

- This classification is also known as Flynn's taxonomy, proposed by Michael J. Flynn in 1966.

### Operations Executed by CPU (SISD)
- Fetch: fetches the data and instructions from a memory area call register
- Decode: The CPU decodes tbe Instructions
- Execute: The instruction is carried out on the data and the result of the operation is stored in another register

### The SISD Architecture Schema

    [Control] --- Instruction ---> [Processor] <--- Data --- [Memory]

- The algorithms run on this kind of architecture are **sequential** or **serial**
- No parallelism (single CPU)

### The Main Elements of SISD Architecture (Von Neumann Architecture)
- Central memory unit: to store for both instructions and data
- CPU: decodes the instructions from memory and sequentially implements them
- The I/O system: refers to the input/output data

### The MISD Architecture Schema

    [           ] --- Data ---> [Processor 1] <--- Instruction 1 --- [Control 1]
    [   Memory  ] --- Data ---> [Processpr 2] <--- Instruction 2 --- [Control 2]
    [           ] --- Data ---> [Processor 3] <--- Instruction N --- [Control N]

- Each processor shares a single memory unit
- In each clock cycle, the data received from the memory by all processors simultaneously
- Instruction level parallelism: performing several operations on the same piece of data
- This architecture is for special purposes, such as data encryption (**more of an intellectual exercise than a practicle configuration**)

### SIMD
- A SIMD computer consists of **'n'** identical processors, each with its own local memory, where it is possible to store data
- Data level parallelism: the processors work simultaneously on each step and execute the same instruction, but on different data elements
- The algorithms for these computer are relatively easy to design, analyze and implement
- SIMD is much more versatile than MISD but the limitation is that problem should be devided into identical sub-problems.
- Connection Machine (1985 Thinking Machine) and MPP(NASA - 1983)
- GPU programming in the folloing section follows this paragon

### The MIMD Architecture Schema

    [Control 1] --- Instruction 1 ---> [Processor 1] <--- Data --- [                    ]
    [Control 2] --- Instruction 2 ---> [Processor 2] <--- Data --- [    Shared Memory   ]
    [Control N] --- Instruction 3 ---> [Processor 3] <--- Data --- [   (Interconnected) ]

- This architecture is more general and more powerful according to Flynn's classification
- Each processor has its own control unit and local memory
- Each processor is under the control of flow of instructions issued by own control unit
- The parallelism is achieved by help of **thread** and all processors
- The processor usually runs **asychronously**
- The limitation is that **asychronous algorithm is difficult to desgin, analyze and implement**.

## Reference:

1. **Python Parallel Programming Solutions**
by Giancarlo Zaccone, Packt Publishing, 2016

2. **Python High Performance - Second Edition**
by Gabriele Lanaro, Packt Publishing, May 2017
