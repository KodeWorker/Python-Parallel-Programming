## Memory Organization (Continued)

### The Distributed Memory Architecture Schema

    [ Processor + cache ]   [ Processor + cache ]   [ Processor + cache ]   [ Processor + cache ]
    [ mem | I/O ]           [ mem | I/O ]           [ mem | I/O ]           [ mem | I/O ]
            │                       │                       │                       │
    ┌───────────────────────────────────────────────────────────────────────────────────────────┐
    │                                    Interconnection Network                                │
    └───────────────────────────────────────────────────────────────────────────────────────────┘
            │                       │                       │                       │
    [ mem | I/O ]           [ mem | I/O ]           [ mem | I/O ]           [ mem | I/O ]
    [ Processor + cache ]   [ Processor + cache ]   [ Processor + cache ]   [ Processor + cache ]

- Sometimes people refer this architecture as multicomputer     

### Advantages of Distributed Memory Architecture
- No conflicts at the level of the communication bus or switch
- No ontrinsic limit to the number of processors
- The size of the system is only limited by the network used to connect the processors
- There are no problems of cache coherency

### Disadvantages of Distributed Memory Architecture
- The communication between processors is more difficult to implement
- If a processor requires data in the memory of another processor, the two processors should necessarily exchange messages via the message-passing protocol
- To build an send a message from one processor to another takes time
- Any processor should be stopped in order to manage the messages received from other processors
- A program designed to work on a distributed memory machine must be organized as a set of independent tasks

### Basic Message passing

    ┌─────────────┐          ┌─────────────┐   
    │   Memory    │          │   Memory    │
    │          ───┼──────────┼───          │
    │ ┌─────────┐ │          │ ┌─────────┐ │
    │ │  data   │ │          │ │  data   │ │
    │ └─────────┘ │          │ └─────────┘ │
    └─────────────┘          └─────────────┘

### Main Features of Distributed Memory Systems
- Memory is physically distributed between processors
- Synchronization is achieved by moving data between processors
- The subdivision of data in the local memories affects the performance of the machine
- The message-passing protocol is used

### MPP (Mass Parallel Processing) Machines
- Composed of hundreds of processors that are connected by a communication network
- The fastest computers in the world are based on these architectures
- Examples: Earth Simulator, Blue Gene, ASCI White, ASCI Red, and ASCI Purple and Red Storm

### Cluster of Workstation Architecture
- Based on a classical computers that are connected by communication network
- We define a node as single computing unit, data is transparent (accessible from any node).

### Types of Clusters
- The fail-over cluster:
nodes are constantly monitored. If any node fails, another node takes over.
- The load balancing cluster:
The job is distributed to the node with less activity. This ensures the less time to complete the process.
- The high-performance comuting cluster
The job is parallelized to multiple mahines.

### The Heterogeneous Architecture Schema

    CPU                                     GPU
    ┌───────────────────┐                   ┌────────────────────┐
    │ [Core 1] [Core 2] │                   │ [Multiprocessor 1] │
    │ [Core 3] [Core 4] │                   │ [Multiprocessor 2] │
    └─────────┬─────────┘                   │           .        │
              │                             │ [Multiprocessor N] │
              │                             └───────────┬────────┘
    <─────────┴─────────────────────────────────────────┴────────>

- Shares physical/Virtual memory
- Achieved by software: CUDA, OpenCL

### Heterogeneous architectures
- Applications can create data structures in a single address space
- Applications can send a job to the device hardware appropriate for the resolution of the task
- Several processing tasks can operate safely on the same regions (**Atomic Operations**)
- We can optimize the interaction of CPU and GPU

## Reference:

1. **Python Parallel Programming Solutions**
by Giancarlo Zaccone, Packt Publishing, 2016

2. **Python High Performance - Second Edition**
by Gabriele Lanaro, Packt Publishing, May 2017
