## (**1.5**) Designing a Parallel Program
- Task decomposition
- Task assignment
- Understanding agglomeration
- Mapping

### Task Decomposition
- The software program is split into tasks or a set of instructions to implement parallelism
- There are two methods to do this sub-division:
    - Domain decomposition: The data of the problems is decomposed
    - Functional decomposition: The problem is split into tasks

### Task Assignment
- Mechanism by which the task will be distributed among the various processes is specified
- It establishes the distribution of workload among the various processors (is crucial to reduce idle time )
- The programmer takes into account the possible heterogeneoty of the system
- For greater efficiency of parallelization, it is necessary to limit communication (main source of slow down and consumption of resources)

## Agglomeration
- The process of combining smaller tasks with larger ones in order to improve performance
- Tasks have to be communicated to the processor or thread
- Most communication has costs that are not only proportional with the amount of data transferredm but also incur a fixed cost for every communication operation (cost of TCP connection)
- If the tasks are too small, this fixed cost can easily make the design inefficient

### Mapping Probelm
- In the mapping stage of the parallel algorithm design process, we specify where each task is to be executed
- The goal is to minimize the total execution time
- You must often make tradeoffs, as the two main strategies often conflict with each other which are:
    1. The tasks that communicate frequently should be placed in the same processor to increase locality
    2. The tasks that can be executed concurrently should be placed in different processors to enhance concurrency
- This problem is NP-complete

### Mapping
- No polynomial time solutions to the problem in the general case exist
- For tasks of equal size and tasks with easily identified communication patterns, the mapping is straightforward
- Load balancing algorithms - Identify agglomeration and mapping strategies during runtime
- The hardest problems are those number of communications or the number of tasks change during execution -> Dynamic load balancing
- Dynamic load balancing algorithms - Run periodically during the execution

### Dynamic Mapping
- Global algorithms require global knowledge of the computation being performed which often adds a lot of overhead
- Local algorithms rely on information that is local to the task in question, which reduces overhead
- A tsk-scheduling algorithm is often used that simply maps tasks to precessors as they become idle
- Tasks are placed in task pool and are taken from it by workers

### Three Common Approaches in Dynamic Mapping Model
- Manager/worker: Basic dynamic mapping scheme in which all the workers connect to a centralized manager
- Hierarchical manager/worker: This is the variant of a manager/worker that has a semi-distributed layout; workers are split into groups, each with their own manager
- Decentralize: Each processor maintains its own task pool and communicates with other processors in order ro request tasks

# Reference:

1. **Python Parallel Programming Solutions**
by Giancarlo Zaccone, Packt Publishing, 2016

2. **Python High Performance - Second Edition**
by Gabriele Lanaro, Packt Publishing, May 2017
